from pathlib import Path
import tkinter as tk
from tkinter import *
from collections import deque, Counter
import time

import cv2
import numpy as np
from PIL import Image, ImageTk
try:
    from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam
except ImportError:
    from keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D, BatchNormalization
    from keras.models import Sequential
    from keras.optimizers import Adam

SCRIPT_DIR = Path(__file__).resolve().parent
BASE_DIR = SCRIPT_DIR.parent
WEIGHTS_PATH = SCRIPT_DIR / "emotion_model.weights.h5"
# Th·ª≠ load best model tr∆∞·ªõc, n·∫øu kh√¥ng c√≥ th√¨ d√πng model th∆∞·ªùng
BEST_WEIGHTS_PATH = SCRIPT_DIR / "emotion_model_best.weights.h5"
CASCADE_PATH = Path(cv2.data.haarcascades) / "haarcascade_frontalface_default.xml"
EMOJI_DIR = BASE_DIR / "emojis" / "emojis"
IMG_SIZE = (48, 48)

# T·ªëi ∆∞u h√≥a: Prediction smoothing v√† frame skipping
PREDICTION_SMOOTHING_SIZE = 5  # S·ªë l∆∞·ª£ng predictions ƒë·ªÉ l√†m m∆∞·ª£t
PREDICTION_INTERVAL = 3  # Ch·ªâ predict m·ªói N frames ƒë·ªÉ tƒÉng t·ªëc
MIN_FACE_SIZE = 30  # K√≠ch th∆∞·ªõc khu√¥n m·∫∑t t·ªëi thi·ªÉu
CONFIDENCE_THRESHOLD = 0.3  # Ng∆∞·ª°ng confidence t·ªëi thi·ªÉu

# H√†m t·∫°o model v·ªõi architecture c≈© (t∆∞∆°ng th√≠ch v·ªõi weights c≈©)
def create_old_model():
    """T·∫°o model v·ªõi architecture c≈© (√≠t layers h∆°n)"""
    model = Sequential(
        [
            # Block 1
            Conv2D(32, kernel_size=(3, 3), activation="relu", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),
            BatchNormalization(),
            Conv2D(64, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Block 2 - Architecture c≈© (√≠t layers h∆°n)
            Conv2D(128, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            MaxPooling2D(pool_size=(2, 2)),
            Conv2D(128, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Block 3 - Architecture c≈©
            Conv2D(256, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Dense layers
            Flatten(),
            Dense(1024, activation="relu"),
            BatchNormalization(),
            Dropout(0.5),
            Dense(512, activation="relu"),
            BatchNormalization(),
            Dropout(0.5),
            Dense(7, activation="softmax"),
        ]
    )
    model.compile(
        loss="categorical_crossentropy",
        optimizer=Adam(learning_rate=1e-4, decay=1e-6),
        metrics=["accuracy"],
    )
    return model

# H√†m t·∫°o model v·ªõi architecture m·ªõi (ƒë√£ c·∫£i thi·ªán)
def create_new_model():
    """T·∫°o model v·ªõi architecture m·ªõi (nhi·ªÅu layers h∆°n)"""
    model = Sequential(
        [
            # Block 1
            Conv2D(32, kernel_size=(3, 3), activation="relu", input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),
            BatchNormalization(),
            Conv2D(64, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Block 2 - Th√™m m·ªôt Conv layer ƒë·ªÉ tƒÉng capacity
            Conv2D(128, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            Conv2D(128, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Block 3 - TƒÉng filters v√† th√™m layer
            Conv2D(256, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            Conv2D(256, kernel_size=(3, 3), activation="relu"),
            BatchNormalization(),
            MaxPooling2D(pool_size=(2, 2)),
            Dropout(0.25),
            
            # Dense layers
            Flatten(),
            Dense(1024, activation="relu"),
            BatchNormalization(),
            Dropout(0.5),
            Dense(512, activation="relu"),
            BatchNormalization(),
            Dropout(0.5),
            Dense(7, activation="softmax"),
        ]
    )
    model.compile(
        loss="categorical_crossentropy",
        optimizer=Adam(learning_rate=1e-4, decay=1e-6),
        metrics=["accuracy"],
    )
    return model

# Load weights if they exist, otherwise show error
# ∆Øu ti√™n load best model n·∫øu c√≥
weights_to_load = BEST_WEIGHTS_PATH if BEST_WEIGHTS_PATH.exists() else WEIGHTS_PATH

if not weights_to_load.exists():
    print(f"ERROR: Model weights not found at {weights_to_load}")
    print("Please run train.py first to train the model.")
    exit(1)

# Th·ª≠ load v·ªõi architecture m·ªõi tr∆∞·ªõc, n·∫øu fail th√¨ d√πng architecture c≈©
emotion_model = None
model_version = None

try:
    print(f"Attempting to load with NEW architecture from {weights_to_load}...")
    emotion_model = create_new_model()
    emotion_model.load_weights(weights_to_load)
    model_version = "NEW"
    print(f"‚úì Successfully loaded NEW model architecture from {weights_to_load}")
except (ValueError, Exception) as e:
    print(f"‚úó Failed to load with NEW architecture: {str(e)[:100]}...")
    print(f"Attempting to load with OLD architecture from {weights_to_load}...")
    try:
        emotion_model = create_old_model()
        emotion_model.load_weights(weights_to_load)
        model_version = "OLD"
        print(f"‚úì Successfully loaded OLD model architecture from {weights_to_load}")
        print("‚ö† WARNING: Using OLD model architecture. For better accuracy, please retrain with:")
        print("   python train.py")
    except Exception as e2:
        print(f"‚úó ERROR: Failed to load model weights: {str(e2)}")
        print("Please check if the weights file is corrupted or run train.py to retrain.")
        exit(1)

print(f"Model version: {model_version}")
print("Optimizations enabled: frame skipping, prediction smoothing, histogram equalization")

cv2.ocl.setUseOpenCL(False)

emotion_dict = {
    0: "Angry",
    1: "Disgust",
    2: "Fear",
    3: "Happy",
    4: "Neutral",
    5: "Sad",
    6: "Surprise",
}

emoji_dist = {
    0: EMOJI_DIR / "angry.png",
    1: EMOJI_DIR / "disgusted.png",
    2: EMOJI_DIR / "fearful.png",
    3: EMOJI_DIR / "happy.png",
    4: EMOJI_DIR / "neutral.png",
    5: EMOJI_DIR / "sad.png",
    6: EMOJI_DIR / "surpriced.png",
}

face_detector = cv2.CascadeClassifier(str(CASCADE_PATH))
cap1 = cv2.VideoCapture(0)
if not cap1.isOpened():
    print("Cannot open the camera.")

# T·ªëi ∆∞u h√≥a camera settings
cap1.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap1.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
cap1.set(cv2.CAP_PROP_FPS, 30)

show_text = [0]
frame_count = [0]  # ƒê·∫øm frame ƒë·ªÉ skip prediction
prediction_history = deque(maxlen=PREDICTION_SMOOTHING_SIZE)  # L∆∞u l·ªãch s·ª≠ predictions
last_prediction_time = [time.time()]  # Th·ªùi gian prediction cu·ªëi c√πng


def show_vid():
    if not cap1.isOpened():
        lmain.after(500, show_vid)
        return

    flag1, frame1 = cap1.read()
    if not flag1:
        lmain.after(10, show_vid)
        return

    frame1 = cv2.resize(frame1, (600, 500))
    gray_frame = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    
    # T·ªëi ∆∞u face detection: tƒÉng scaleFactor v√† ƒëi·ªÅu ch·ªânh minNeighbors ƒë·ªÉ nhanh h∆°n
    # minSize gi√∫p b·ªè qua c√°c khu√¥n m·∫∑t qu√° nh·ªè
    num_faces = face_detector.detectMultiScale(
        gray_frame, 
        scaleFactor=1.2,  # Gi·∫£m t·ª´ 1.3 xu·ªëng 1.2 ƒë·ªÉ ph√°t hi·ªán t·ªët h∆°n
        minNeighbors=4,   # Gi·∫£m t·ª´ 5 xu·ªëng 4 ƒë·ªÉ nhanh h∆°n nh∆∞ng v·∫´n ch√≠nh x√°c
        minSize=(MIN_FACE_SIZE, MIN_FACE_SIZE),  # B·ªè qua khu√¥n m·∫∑t qu√° nh·ªè
        flags=cv2.CASCADE_SCALE_IMAGE
    )

    # Ch·ªâ predict m·ªói N frames ƒë·ªÉ tƒÉng t·ªëc ƒë·ªô
    should_predict = (frame_count[0] % PREDICTION_INTERVAL == 0)
    frame_count[0] += 1
    
    current_time = time.time()
    # ƒê·∫£m b·∫£o kh√¥ng predict qu√° th∆∞·ªùng xuy√™n (t·ªëi thi·ªÉu 0.1 gi√¢y)
    if should_predict and (current_time - last_prediction_time[0]) > 0.1:
        for (x, y, w, h) in num_faces:
            # Ch·ªâ x·ª≠ l√Ω khu√¥n m·∫∑t l·ªõn nh·∫•t n·∫øu c√≥ nhi·ªÅu khu√¥n m·∫∑t
            if len(num_faces) > 1:
                # S·∫Øp x·∫øp theo di·ªán t√≠ch v√† l·∫•y khu√¥n m·∫∑t l·ªõn nh·∫•t
                areas = [w * h for (_, _, w, h) in num_faces]
                largest_idx = np.argmax(areas)
                (x, y, w, h) = num_faces[largest_idx]
            
            # V·∫Ω rectangle ƒë·∫πp h∆°n v·ªõi m√†u gradient (simulated v·ªõi 2 m√†u)
            cv2.rectangle(frame1, (x, y - 50), (x + w, y + h + 10), (233, 69, 96), 3)  # Pink/red border
            cv2.rectangle(frame1, (x + 2, y - 48), (x + w - 2, y + h + 8), (83, 52, 131), 2)  # Purple inner border
            
            roi_gray_frame = gray_frame[y : y + h, x : x + w]
            
            # C·∫£i thi·ªán preprocessing: th√™m histogram equalization ƒë·ªÉ tƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n
            roi_gray_frame = cv2.equalizeHist(roi_gray_frame)
            
            cropped_img = cv2.resize(roi_gray_frame, IMG_SIZE)
            cropped_img = np.expand_dims(np.expand_dims(cropped_img, -1), 0) / 255.0
            
            # S·ª≠ d·ª•ng predict_on_batch thay v√¨ predict ƒë·ªÉ nhanh h∆°n
            prediction = emotion_model.predict_on_batch(cropped_img)
            maxindex = int(np.argmax(prediction))
            confidence = float(np.max(prediction))
            
            # Ch·ªâ c·∫≠p nh·∫≠t n·∫øu confidence ƒë·ªß cao
            if confidence >= CONFIDENCE_THRESHOLD:
                prediction_history.append(maxindex)
                # L√†m m∆∞·ª£t: l·∫•y mode (gi√° tr·ªã xu·∫•t hi·ªán nhi·ªÅu nh·∫•t) c·ªßa l·ªãch s·ª≠
                if len(prediction_history) > 0:
                    most_common = Counter(prediction_history).most_common(1)[0][0]
                    show_text[0] = most_common
                else:
                    show_text[0] = maxindex
            else:
                # N·∫øu confidence th·∫•p, gi·ªØ nguy√™n prediction c≈©
                if len(prediction_history) > 0:
                    show_text[0] = prediction_history[-1]
            
            last_prediction_time[0] = current_time
            
            # Hi·ªÉn th·ªã emotion v√† confidence v·ªõi style ƒë·∫πp h∆°n
            emotion_text = emotion_dict.get(show_text[0], "Unknown")
            confidence_text = f"{confidence:.1%}"
            full_text = f"{emotion_text} {confidence_text}"
            
            # Background cho text (semi-transparent effect v·ªõi rectangle)
            text_size = cv2.getTextSize(full_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
            cv2.rectangle(frame1, 
                         (x, y - 35), 
                         (x + text_size[0] + 10, y - 5), 
                         (26, 26, 46), -1)  # Dark background
            cv2.rectangle(frame1, 
                         (x, y - 35), 
                         (x + text_size[0] + 10, y - 5), 
                         (233, 69, 96), 2)  # Border
            
            # Text v·ªõi m√†u ƒë·∫πp
            cv2.putText(frame1, full_text, 
                       (x + 5, y - 15), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
            break  # Ch·ªâ x·ª≠ l√Ω khu√¥n m·∫∑t ƒë·∫ßu ti√™n/l·ªõn nh·∫•t
    else:
        # V·∫´n v·∫Ω rectangle ngay c·∫£ khi kh√¥ng predict
        for (x, y, w, h) in num_faces:
            cv2.rectangle(frame1, (x, y - 50), (x + w, y + h + 10), (233, 69, 96), 3)
            cv2.rectangle(frame1, (x + 2, y - 48), (x + w - 2, y + h + 8), (83, 52, 131), 2)
            if len(prediction_history) > 0:
                emotion_text = emotion_dict.get(show_text[0], "Unknown")
                text_size = cv2.getTextSize(emotion_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)[0]
                cv2.rectangle(frame1, 
                             (x, y - 35), 
                             (x + text_size[0] + 10, y - 5), 
                             (26, 26, 46), -1)
                cv2.rectangle(frame1, 
                             (x, y - 35), 
                             (x + text_size[0] + 10, y - 5), 
                             (233, 69, 96), 2)
                cv2.putText(frame1, emotion_text, 
                           (x + 5, y - 15), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)

    pic = cv2.cvtColor(frame1, cv2.COLOR_BGR2RGB)
    img = Image.fromarray(pic)
    imgtk = ImageTk.PhotoImage(image=img)
    lmain.imgtk = imgtk
    lmain.configure(image=imgtk)
    lmain.after(10, show_vid)


def show_vid2():
    emotion_index = show_text[0]
    emoji_path = emoji_dist.get(emotion_index)
    if emoji_path and emoji_path.exists():
        frame2 = cv2.imread(str(emoji_path))
        frame2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2RGB)
        img2 = Image.fromarray(frame2)
        
        # Resize emoji ƒë·ªÉ v·ª´a v·ªõi container (250x250 pixels)
        emoji_size = (250, 250)
        # S·ª≠ d·ª•ng LANCZOS resampling cho ch·∫•t l∆∞·ª£ng t·ªët nh·∫•t
        try:
            img2 = img2.resize(emoji_size, Image.Resampling.LANCZOS)
        except AttributeError:
            # Fallback cho phi√™n b·∫£n PIL c≈©
            img2 = img2.resize(emoji_size, Image.LANCZOS)
        
        imgtk2 = ImageTk.PhotoImage(image=img2)
        lmain2.imgtk2 = imgtk2
        lmain2.configure(image=imgtk2)

    # C·∫≠p nh·∫≠t emotion text v·ªõi font ƒë·∫πp h∆°n (ƒë√£ ƒë∆∞·ª£c set trong main)
    emotion_name = emotion_dict.get(emotion_index, "Neutral")
    lmain3.configure(text=emotion_name)
    lmain2.after(200, show_vid2)


def on_close():
    if cap1.isOpened():
        cap1.release()
    cv2.destroyAllWindows()
    root.destroy()


if __name__ == "__main__":
    root = tk.Tk()
    
    # M√†u s·∫Øc theme ƒë·∫πp h∆°n
    BG_COLOR = "#1a1a2e"  # Dark blue-gray
    SECONDARY_BG = "#16213e"  # Slightly lighter
    ACCENT_COLOR = "#0f3460"  # Blue accent
    TEXT_COLOR = "#e94560"  # Pink/red for headings
    TEXT_SECONDARY = "#ffffff"  # White for main text
    TEXT_TERTIARY = "#a8a8a8"  # Light gray
    BORDER_COLOR = "#533483"  # Purple border
    BUTTON_BG = "#e94560"  # Pink/red button
    BUTTON_HOVER = "#c73650"  # Darker pink on hover
    
    root["bg"] = BG_COLOR
    root.title("üé≠ Emoji Creator - AI Emotion Detection")
    root.geometry("1400x900+100+10")
    root.protocol("WM_DELETE_WINDOW", on_close)
    
    # T·∫°o frame ch√≠nh v·ªõi padding
    main_frame = tk.Frame(root, bg=BG_COLOR)
    main_frame.pack(fill=tk.BOTH, expand=True, padx=20, pady=20)
    
    # Header section v·ªõi gradient effect (simulated) - cƒÉn gi·ªØa
    header_frame = tk.Frame(main_frame, bg=SECONDARY_BG, relief=tk.RAISED, bd=2)
    header_frame.pack(fill=tk.X, pady=(0, 20))
    
    # Try to load logo, if not found, create beautiful text heading
    logo_path = SCRIPT_DIR / "logo.png"
    if logo_path.exists():
        img = ImageTk.PhotoImage(Image.open(str(logo_path)))
        heading = Label(header_frame, image=img, bg=SECONDARY_BG)
        heading.pack(pady=15)
    else:
        # Beautiful text heading v·ªõi gradient effect (simulated) - cƒÉn gi·ªØa
        heading = Label(
            header_frame, 
            text="üé≠ Emoji Creator", 
            font=("Segoe UI", 36, "bold"), 
            bg=SECONDARY_BG, 
            fg=TEXT_COLOR
        )
        heading.pack(pady=15)
    
    heading2 = Label(
        header_frame, 
        text="AI Emotion Detection System", 
        font=("Segoe UI", 18, "italic"), 
        bg=SECONDARY_BG, 
        fg=TEXT_TERTIARY
    )
    heading2.pack(pady=(0, 15))
    
    # Content area v·ªõi grid layout c√¢n ƒë·ªëi
    content_frame = tk.Frame(main_frame, bg=BG_COLOR)
    content_frame.pack(fill=tk.BOTH, expand=True, pady=10)
    
    # Container cho left v√† right side - cƒÉn gi·ªØa
    main_content = tk.Frame(content_frame, bg=BG_COLOR)
    main_content.pack(expand=True, fill=tk.BOTH)
    
    # Left side - Video feed v·ªõi border ƒë·∫πp
    left_container = tk.Frame(main_content, bg=BG_COLOR)
    left_container.pack(side=tk.LEFT, expand=True, fill=tk.BOTH, padx=(0, 15))
    
    video_frame = tk.Frame(left_container, bg=BORDER_COLOR, relief=tk.RAISED, bd=3)
    video_frame.pack(expand=True, fill=tk.BOTH, padx=10, pady=10)
    
    video_label_frame = tk.Label(
        video_frame, 
        text="üìπ Live Camera Feed", 
        font=("Segoe UI", 12, "bold"), 
        bg=BORDER_COLOR, 
        fg=TEXT_SECONDARY
    )
    video_label_frame.pack(pady=8)
    
    lmain = tk.Label(
        master=video_frame, 
        bg="#000000",
        relief=tk.SUNKEN,
        bd=2
    )
    lmain.pack(padx=10, pady=(0, 10), fill=tk.BOTH, expand=True)
    
    # Right side - Emotion display
    right_container = tk.Frame(main_content, bg=BG_COLOR)
    right_container.pack(side=tk.RIGHT, expand=True, fill=tk.BOTH, padx=(15, 0))
    
    emotion_frame = tk.Frame(right_container, bg=BG_COLOR)
    emotion_frame.pack(expand=True, fill=tk.BOTH, padx=10, pady=10)
    
    # Emotion label v·ªõi style ƒë·∫πp - cƒÉn gi·ªØa
    emotion_label_frame = tk.Frame(emotion_frame, bg=SECONDARY_BG, relief=tk.RAISED, bd=2)
    emotion_label_frame.pack(fill=tk.X, pady=(0, 15))
    
    emotion_title = tk.Label(
        emotion_label_frame,
        text="üòä Detected Emotion",
        font=("Segoe UI", 14, "bold"),
        bg=SECONDARY_BG,
        fg=TEXT_COLOR
    )
    emotion_title.pack(pady=10)
    
    # Emotion name v·ªõi style l·ªõn v√† ƒë·∫πp - cƒÉn gi·ªØa
    lmain3 = tk.Label(
        master=emotion_frame, 
        text="Neutral",
        font=("Segoe UI", 42, "bold"),
        bg=BG_COLOR,
        fg=TEXT_COLOR,
        relief=tk.RAISED,
        bd=3,
        padx=30,
        pady=15
    )
    lmain3.pack(pady=15)
    
    # Emoji display v·ªõi border ƒë·∫πp v√† k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh - cƒÉn gi·ªØa
    emoji_wrapper = tk.Frame(emotion_frame, bg=BG_COLOR)
    emoji_wrapper.pack(expand=True, fill=tk.BOTH, pady=10)
    
    emoji_container = tk.Frame(emoji_wrapper, bg=BORDER_COLOR, relief=tk.RAISED, bd=3)
    emoji_container.pack(expand=True)  # CƒÉn gi·ªØa trong wrapper
    
    emoji_label = tk.Label(
        emoji_container,
        text="üé≠ Emoji Preview",
        font=("Segoe UI", 12, "bold"),
        bg=BORDER_COLOR,
        fg=TEXT_SECONDARY
    )
    emoji_label.pack(pady=8)
    
    # Label v·ªõi k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh ƒë·ªÉ hi·ªÉn th·ªã emoji ƒë·∫ßy ƒë·ªß
    lmain2 = tk.Label(
        master=emoji_container,
        bg="#000000",
        relief=tk.SUNKEN,
        bd=2,
        width=250,
        height=250
    )
    lmain2.pack(padx=15, pady=(0, 15))
    
    # Status bar - cƒÉn gi·ªØa text
    status_frame = tk.Frame(main_frame, bg=SECONDARY_BG, relief=tk.SUNKEN, bd=1)
    status_frame.pack(side=tk.BOTTOM, fill=tk.X, pady=(0, 10))
    
    status_text = f"‚úì Model loaded: {model_version} | Optimizations: Frame skipping, Smoothing, Histogram EQ"
    status_label = tk.Label(
        status_frame,
        text=status_text,
        font=("Segoe UI", 9),
        bg=SECONDARY_BG,
        fg=TEXT_TERTIARY
    )
    status_label.pack(padx=10, pady=5)
    
    # Footer v·ªõi button ƒë·∫πp - cƒÉn gi·ªØa
    footer_frame = tk.Frame(main_frame, bg=BG_COLOR)
    footer_frame.pack(side=tk.BOTTOM, fill=tk.X, pady=(10, 0))
    
    # Button v·ªõi style ƒë·∫πp h∆°n - cƒÉn gi·ªØa
    quit_button = tk.Button(
        footer_frame,
        text="‚ùå Quit Application",
        command=on_close,
        font=("Segoe UI", 16, "bold"),
        bg=BUTTON_BG,
        fg=TEXT_SECONDARY,
        activebackground=BUTTON_HOVER,
        activeforeground=TEXT_SECONDARY,
        relief=tk.RAISED,
        bd=3,
        padx=30,
        pady=10,
        cursor="hand2"
    )
    quit_button.pack(pady=10)
    
    show_vid()
    show_vid2()
    root.mainloop()
